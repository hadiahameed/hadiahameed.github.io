---
layout: post
title: "Episode 1: Multi-output forecasting"
date: 2019-09-01
---

This week I read the following paper [2], and this is everything I learned from it:  

## Vocabulary:  

+ **Signal-step forecasting problem**: Estimate a future value of the signal for a single time instant using past values. e.g. blood glucose measurement.  
+ **Multi-step forecasting problem**: Estimating multiple values within a future time horizon, recursively.  
+ **Multi-output forecasting**: Estimating multiple values within a future time horizon, all at the same time instead of   recursively.  

## Concepts:  
+ Why does multi-step forecasting (MSF) often have a poor long-term performance?  
In MSF, we use the current prediction to make the next prediction recursively and any error introduced at one stage can enter a positive feedback loop (accumulate over time).
  
+ What is the problem with multi-output forecasting?   
It may not adequately capture dependencies among predictions.
  
+ What is the difference between autoregressive (ARN) and recurrent neural networks (RNN)?   
RNNs use all the previous outputs for predicting the future value through hidden states but ARN use a limited number of past values given directly as input to the next cell, like any typical feed-forward network. There is an interesting set of blogposts explaining this concept [3],[4] 

ARN architecture             |  RNN architecture
:-------------------------:|:-------------------------:
![Source: Source: https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif){:height="20%" width="40%"}  |  ![Source: https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359](https://miro.medium.com/max/1722/1*SAuwuiKBhzR4tBhr54mYkA.png){:height="20%" width="40%"}
  
+ What is ARIMA and how is it used in time-series forecasting? ARIMA (Autoregressive integrated moving average)[5] 
 - AR:  Auto-regressive: captures dependencies between present and past signal values.
 - I: Integrated: Makes the time-series stationary by differentiating the observations. 
 - MA: Moving average: Takes a moving average of the observations and incorporates residual error (= expected_value â€“ predicted_value)
   
 + What is early stopping in machine learning?  
 - It is a form of implicit regularization that is done to achieve a balance between underfitting and overfitting in order to improve generalization. Approaches:
 - Change training epochs. Drawback: One has to train and discard several models for each value of epoch in order to find the best epoch.
 - If the performance on validation set start degrading after a particular epoch, stop the training.  
  
  
## References:
[1] [Machine Learning Mastery]({{ https://machinelearningmastery.com }})
[2] [Deep Multi-output Forecasting]({{ https://arxiv.org/abs/1806.05357 }})  
[3] [Recurrent Neural Networks]({{ https://bair.berkeley.edu/blog/2018/08/06/recurrent/ }})  
[4] [Autoregressive Models]({{ https://eigenfoo.xyz/deep-autoregressive-models/ }})  
[5] [ARIMA]({{ https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/ }})  
 





