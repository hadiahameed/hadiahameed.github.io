---
layout: post
title: "Episode 10: Notes from Deep Learning Course (Part I)"
description: Taught by Yann LeCunn and Alfredo Canziani at NYU
tags: [machinelearning, deeplearning, nyu, academia]
date: 2020-12-05

---

This is part of a series of posts on some of my notes from course on Deep Learning taught by Prof. Yann LeCunn at NYU in Spring 2020 [1].

## Concepts:  

+ <span style="color:blue">What are 0th order methods?</span>       

  These are gradient-free methods used to minimize a cost function. We use them when the cost function is 

  - not differentiable and has sharp discontinuities so finding its gradient is not feasible. 
  - Multimodal
  - Has mixed variables. 
  - Not known, when we are not sure what it looks like e.g. in reinforcement learning when we are trying to train a robot to ride a bike, our reward function outputs a reward when the robot doesn’t fall and no reward when it falls. We don’t know how to formulate this reward mechanism as a differentiable function, so we use gradient-free methods for optimization.  

  Examples: Genetic Algorithms, Particle Swarm Optimization etc. 

   As opposed to these methods, we have gradient-based methods such as gradient descent which can be used to optimize functions which have a known closed form, are continuous and differentiable almost everywhere. More fun stuff on it here [2].   

  

+ <span style="color:blue">What is Kaiming trick of weight initialization?</span>   

   Initializing weights is the one of the key factors that ensure that the neural network (NN) gives optimal results and converges smoothly. If we choose very large initial weights, it can lead to gradient explosion and if too small weights are chosen at the start, it can cause vanishing gradient problem.

  Kaiming trick states that when initializing an NN, we can draw these weights randomly from a normal distribution and scale them by a factor of  $\sqrt{\frac{2}{input_size}}$ .

  So if a layer has 10 number of input nodes going into it, we can scale its initial weights (drawn from a normal distribution) by a factor of  \sqrt{\frac{2}{10}}  . This trick works only for layers with ReLU as an activation layer. 

   LeCun’ comments: (“We want the variance of the output to be roughly the same as that of inputs. If the inputs to a unit are independent, the variance of the output…will be equal to the sum of the variances of the input weighted by the square of the weights. So if you have n inputs and you want the variance of the output to have the same variance, the weights should be proportional to the inverse square root of the number of inputs.”)

   Another easy-to-understand explanation by James Dellinger, describing the multiplication of a 521x521 weight matrix with 521x1 input vector, is as follows:

  



## Thought of the Week:  

A couple of days ago I came across an article shared by Yann LeCun (the father of Convolutional Neural Networks (CNN) and also a professor at NYU), titled <a href="https://towardsdatascience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf">"The Most Important Court Decision For Data Science and Machine Learning"</a> by Matthew Stewart [4]. This article gives a quick rundown of the famous Authors Guild vs Google case which dragged on for more than ten years, becoming the most important case study in the field of artificial intelligence and machine learning. In 2005, two famous publishers in the US sued Google for using copyrighted books for training their book search algorithms.  

Google books search algorithm is a highly sophisticated machine learning algorithm for optimizing book search for users.  
Recently, I searched for a couple of titles to understand how the algorithm actually worked. When I entered general keywords like "Lab" or "Blood", it didn't list the books based on word frequency in the existing database of books (because that would have simply given me a list of medical books), instead the algorithm also took into account the popularity of the books in other web searches (which is based on PageRank algorithm). For example, for "blood", the book "Bad Blood" by John Carreyrou was among the top results. It is said that Google has scanned more than 15 million books and put them on web [5].

Anyhow, the end result of the famous case between Authors Guild and Google was that the Supreme Court eventually dismissed the lawsuit and Google was officially let off the hook. But the ruling helped establish some important precedents in Artificial intelligence, such as affirming that the *"use of copyrighted material ... to train a discriminative machine-learning algorithms (such as for search purposes) is legal"* [4] But how would that fare in generative machine learning to produce deep fakes, is still an open question. 

 Until next time! 

## References:

[1] [Deep Learning Course by Yann LeCun and Alfredo Canziani](https://atcold.github.io/pytorch-Deep-Learning/)  
[2] [0th order methods](http://adl.stanford.edu/aa222/Lecture_Notes_files/chapter6_gradfree.pdf)  
[3] [Probability concepts explained: Maximum likelihood estimation by Jonny Brooks-Bartlett](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)  
[4] [Python Lists vs Dictionaries: The space-time tradeoff](