---
layout: post
title: "Episode 10: Notes from Deep Learning Course (Part I)"
description: Taught by Yann LeCunn and Alfredo Canziani at NYU
tags: [machinelearning, deeplearning, nyu, academia]
date: 2020-12-05

---

This is part of a series of posts on some of my notes from course on Deep Learning taught by Prof. Yann LeCunn at NYU in Spring 2020 [1].

## Concepts:  

+ <span style="color:blue">What are 0th order methods?</span>       

  These are gradient-free methods used to minimize a cost function. We use them when the cost function is 

  - not differentiable and has sharp discontinuities so finding its gradient is not feasible. 
  - Multimodal
  - Has mixed variables. 
  - Not known, when we are not sure what it looks like e.g. in reinforcement learning when we are trying to train a robot to ride a bike, our reward function outputs a reward when the robot doesn’t fall and no reward when it falls. We don’t know how to formulate this reward mechanism as a differentiable function, so we use gradient-free methods for optimization.  

  Examples: Genetic Algorithms, Particle Swarm Optimization etc. 

   As opposed to these methods, we have gradient-based methods such as gradient descent which can be used to optimize functions which have a known closed form, are continuous and differentiable almost everywhere. More fun stuff on it here [2].   

  

+ <span style="color:blue">What is Kaiming trick of weight initialization?</span>   

   Initializing weights is the one of the key factors that ensure that the neural network (NN) gives optimal results and converges smoothly. If we choose very large initial weights, it can lead to gradient explosion and if too small weights are chosen at the start, it can cause vanishing gradient problem.

  Kaiming trick states that when initializing an NN, we can draw these weights randomly from a normal distribution and scale them by a factor of  <a href="https://www.codecogs.com/eqnedit.php?latex=\sqrt{\frac{2}{input\_size}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sqrt{\frac{2}{input\_size}}" title="\sqrt{\frac{2}{input\_size}}" /></a>

  So if a layer has 10 number of input nodes going into it, we can scale its initial weights (drawn from a normal distribution) by a factor of <a href="https://www.codecogs.com/eqnedit.php?latex=\sqrt{\frac{2}{10}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sqrt{\frac{2}{10}}" title="\sqrt{\frac{2}{10}}" /></a>. This trick works only for layers with ReLU as an activation layer. 

   LeCun’ comments: (“We want the variance of the output to be roughly the same as that of inputs. If the inputs to a unit are independent, the variance of the output…will be equal to the sum of the variances of the input weighted by the square of the weights. So if you have n inputs and you want the variance of the output to have the same variance, the weights should be proportional to the inverse square root of the number of inputs.”)

   Another easy-to-understand explanation by James Dellinger, describing the multiplication of a *521x521* weight matrix with *521x1* input vector, is as follows: [3]

  <img src="{{site.baseurl}}/assets/lecun_i11.png" width="75%" height="75%">  

  

  <img src="{{site.baseurl}}/assets/lecun_i12.png" width="75%" height="75%">  

  

  <img src="{{site.baseurl}}/assets/lecun_i13.png" width="75%" height="75%">                  

## Thought of the Week:  

Generative Pre-trained Transformer 3 (GPT-3) is here! It is now the largest language model that we know of, developed by an independent lab called OpenAI in San Francisco. It has 175 billion parameters and has been trained on millions of text documents including Google books and Wikipedia articles. The underlying principle behind GPT modeling is semi-supervised learning which means to train the model on a large-sized unlabeled data in unsupervised manner and then fine-tune it by supervised learning on small set of labeled examples. []  

Even though GPT-3 was developed for sentence completion tasks by its developers but it turns out that it can also be used to write code! Seems like it's time for us computer scientists to get insecure about our jobs. Check out this interesting article in <a href="https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html"> New York Times</a> to know more fun stuff about GPT-3. 

 Until next time! 

## References:

[1] [Yann LeCun Course](https://atcold.github.io/pytorch-Deep-Learning/)
[2] [0th order methods](http://adl.stanford.edu/aa222/Lecture_Notes_files/chapter6_gradfree.pdf)  
[3] [Kaiming Trick of weight initialization](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)  
[] [GPT-3](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2)