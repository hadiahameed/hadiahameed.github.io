---
layout: post
title: "Episode 6: What in the world is Kullback-Leibler Divergence?"
description: Comparing distributions in machine learning
tags: [statistics, probability, linearalgebra, mathematics, machinelearning]
date: 2019-10-16
---

This week I discovered a new blog on topics related to Bayesian Theory and this is everything I would like to remember from a post related to <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback-Leibler Divergence.</a>  

## Concepts:  
+ <span style="color:blue">What is Kullback-Leibler Divergence used for?</span>  
Instead of taking the actual distribution of observed data, which may be unknown or too complex, we often approximate the distribution. KL-divergence is used to measure how much information we lose by this approximation. 


+ <span style="color:blue">What does entropy mean in the context of information theory (or electrical engineering)?</span>  
If we use log<sub>2</sub>, entropy means the least number of bits we would need to encode the information. For example to encode 8, we would need (log<sub>2</sub>(8) + 1) = 3  + 1 bits. 


+ <span style="color:blue">What parameters do you need for creating binomial distribution?</span>  
Number of trials (e.g. number of coin flips) = n  
Probability of each unique target value (e.g. heads and tails in a coin flip) = p  

<img src="{{site.baseurl}}/assets/bar_plot.png" width="75%" height="55%">    

If we have a population of subjects, and they can have any number of diseases from 0 to 10, as shown in the figure above, then n = 10. We don’t know the probability p for our binomial distribution, so we calculate the mean of our observed data E[x]= &sum_{i=1}^{10} x_ip(x_i) = 4.7. Using E[X] = np, we calculate p which comes out to be 4.7/10 = 0.47. So binomial distribution can be constructed using: 

<img src="{{site.baseurl}}/assets/formula.png" width="35%" height="35%">  
Where k goes from 0 to 10. 

+ <span style="color:blue">Looking at the formula for KL-divergence, how can you define it in terms of expectation?</span> 

<figure>
    <img src="https://miro.medium.com/proxy/1*wzgJUCDsBgtleCIGmCMo5Q.png" width="35%" height="35%">
    <figcaption>Source: <a href="https://towardsdatascience.com/part-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d">Medium, A new Tool to your Toolkit, KL Divergence at Work</a></figcaption>
</figure>    

P is actual distribution and q is approximated distribution. Since expectation E[x] is defined as the product of x and its probability, K-L divergence is the expectation of the difference between log of actual distribution and log of approximated distribution.  

+ <span style="color:blue">How is KL-divergence used as a cost function in optimization?</span>  
We tune the parameters for our new approximated distribution by minimizing the KL-divergence.  We can do this by plotting the KL-divergence value against different values of parameter p for our new distribution and choosing the value for which KL-Divergence is the least. This can also be extended to higher dimensional models with lots of parameters. 

+ <span style="color:blue">What are some of the constraints on/properties of K-L Divergence metric? [3]?</span>  
-	It is non-symmetric (“reversing the roles of the two arguments in the KL divergence can yield substantially different results”)
-	It is always greater than 0. [2]
-	It is only defined if P and Q both sum to 1 and Q(x) > 0 if P(x) > 0.[2]
-	It has an asymptotic behavior. (because of the log terms) 
-	It is also known as relative entropy, or the I-divergence.

+ <span style="color:blue">Important applications and interesting papers on KL-Divergence?</span>  
-	Model Selection, 2007 [3]
-	Speech Recognition, 2004 [4]
-	Speaker Identification/Image Classification, 2004 [5]


## Thought of the Week:  
This week I found myself in a rut when I sat down to organize my Machine Learning notes. Taking a course in Machine Learning, I have come to realize, is like going to Ikea. You like everything and you want to buy everything but you rarely need all those things at the same time. So, earlier this week, I rolled up my sleeves, picked up my data-mining shovel and decided to dig deep into the topic of bias-variance tradeoff, only to end up having a vertigo as I jumped from one concept to another, with a hundred different tabs opened up in my browser. But one good thing that came out of it was discovering a new blog on probability theory called <a href="https://www.countbayesie.com">Count Bayesie"</a> (which is already up on my favorite list of blogs), when I was trying to find out <a href="https://www.countbayesie.com/blog/2019/1/30/a-deeper-look-at-mean-squared-error">how to expand the expecation of mean squared error </a>(geeky segue alert!). Discovering a good resource in data science makes me realize how far I am from where I want to be in this field, but it is also like discovering a new trail along the way, a detour which might make this long, arduous journey plagued with frequent fits of self-scrutiny and imposter syndrome, more fun and exploratory. There is so much to learn and you get a day everyday to do it. What more could one ask for? Happy reading!  

See you next week!
 
## References:
[1] [Kullback-Leibler Divergence Explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)  
[2] [A new divergence measure for basic probability assignment and its applications in extremely uncertain environments](https://onlinelibrary.wiley.com/doi/full/10.1002/int.22066)  
[3] [The AIC Criterion and Symmetrizing the Kullback–Leibler Divergence](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4049836)  
[4] [A New Kullback–Leibler VAD for Speech Recognition in Noise](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1261996)  
[5] [A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications](http://papers.nips.cc/paper/2351-a-kullback-leibler-divergence-based-kernel-for-svm-classification-in-multimedia-applications.pdf)



