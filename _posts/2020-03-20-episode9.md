---
layout: post
title: "Episode 9: ML Notes (Part II): Fundamentals of Decision Tree Theory"
description: Notes from Tom M. Mitchell's book
tags: [machinelearning, theory, datascience, decisiontrees]
date: 2020-03-20
---

These days I am reading the book <a href="http://www.cs.cmu.edu/~tom/mlbook.html">"Machine Learning"</a> by Tom M. Mitchell and I will be documenting some important concepts from its chapters in a series of blog posts.  

Check out <a href="https://hadiahameed.github.io/blog/2020/03/13/episode8">"Part I"</a> with notes from Chapter I,II if you haven't already!  

## [Background]: 
Decision trees iteratively partition the data and generate rules which helps us to make necessary predictions. 
<figure>
    <img src="https://i0.wp.com/www.samtalksml.net/wp-content/uploads/2017/05/image_dt1-1.png?resize=450%2C368&ssl=1" width="55%" height="55%">
    <figcaption>Source: <a href="https://www.samtalksml.net/helping-doctors-validate-decision-trees/">Helping doctors validate decision trees by Samuel Laurence</a></figcaption>
</figure> 

## Concepts:  
+ <span style="color:blue">What are the different decision tree algorithms and how do they differ from each other?</span>[2]  

    **Iterative Dichotomiser 3 (ID3):**  
    - Developed by Ross Quinlan 1986.    
    - Starts with an empty tree. Iteratively constructs a final tree that’s supposedly the shortest tree that is consistent with the training data.     
    - Uses post-pruning to fix overfitting.    
    - Uses information gain to select attributes.    
    - Works only with discrete-valued attributes.    

    **C4.5:**  
    - Successor of ID3 [3].    
    - Works with both discrete and continuous-valued attributes (by partitioning).    
    - Converts ID3 into if-then rules.    
    - Has a special technique for replacing missing values. If an attribute is missing, it assigns probability to each possible value of that attribute [3,4].   

    **CART:**    
    - Similar to C4.5 but in addition to numerical-valued attributes, it can also support numerical target values (regression) instead of just discrete-valued targets.     
    - Creates binary trees.     



+ <span style="color:blue">What are the different ways in which entropy can be understood?</span>  
    - ID3 decision tree algorithm works by finding the best attribute to use at each node of the tree.  
    - It selects attribute with the highest information gain which is a statistical measure.  
    - Information gain uses entropy which is a measure used to characterize the impurity of a set of examples.  
    - Entropy of 0 means all the examples in the set belong to the same target class. (pure data. Impurity is 0).  
    - Entropy of 1 means half the examples belong to one class and the other half belong to the other class. (very impure data).  
    - It is the minimum number of bits needed to encode information about the target class of a given instance.  
    - Information gain of an attribute measures the drop in entropy (impurity) if the data was split using this attribute. It tells us the number of bits saved in encoding the information of the target class, if this attribute was used to split the data.  



+ <span style="color:blue">Why is ID3 an example of inductive learning?</span>   
Constructing a decision tree using ID3 algorithm is essentially a searching problem in which we are finding the best tree from a large set of possible trees (or hypotheses) that best fits the training data. This searching, in case of ID3, is termed as "simple-to-complex", "hill-climbing search". It is called simple-to-complex since we start with an empty tree and end up with the final tree consisting of many nodes. Hill-climbing means that it incrementally moves towards an optimal solution which might be locally and not globally the most optimal solution.  


+ <span style="color:blue">What is the inductive bias of ID3 algorithm?</span>   
It chooses shorter trees over longer trees since it is a simple-to-complex algorithm. Among these shorter trees, it uses the one that places the attribute with the highest information gain at the root.  


+ <span style="color:blue">What is the difference between preference bias and restrictive bias?</span>  
The inductive bias of ID3 algorithm is an example of preference or search bias. This bias is based on its search strategy as it stops searching as soon as it finds the best hypothesis from a complete hypotheses space even though there might be several other equally good or even better hypotheses. This type of bias is called **preference/search bias** as opposed to **restriction or language bias** in which the algorithm continues to search through the entire hypotheses space until it finds all the hypotheses consistent with the data, but the hypotheses space it chooses is based on some assumptions, hence in this case, it is an incomplete hypotheses space. Preference bias is often preferred over restriction bias since it involves a complete set hypotheses instead of an incomplete one.   


+ <span style="color:blue">Give an example of an algorithm that uses both preference and restrictive bias?</span>   
Linear Regression.  
Uses restrictive bias because its hypotheses space consists of only linear functions and not non-linear functions for example, so it is an incomplete hypotheses space.  
Uses preference bias: Because it uses lease mean square algorithm to find optimal solution which does an ordered search through the set of all possible parameter values.   


+ <span style="color:blue">What are some of the limitations of the ID3 algorithm?</span>   
    1. It maintains only a single current hypothesis and cannot tell whether there could have been alternative trees that are also consistent with the training examples.  
    2. It does not have an mechanism to backtrack. For example, if it chooses a certain attribute as a node, it cannot go back and reconsider it. This means that it has a tendency to converge to local and not global optimal solutions. A common remedy to this is post-pruning trees.  


+ <span style="color:blue">What is Occam’s razor?</span>   
This is the inductive bias introduced by William of Occam in 1320. It says, "Prefer the simplest hypotheses that fits the data."  


+ <span style="color:blue">What is reduced-error pruning?</span>   
It is a technique to fix over-fitting in decision trees. After the tree is constructed, each node is considered for pruning, based on the results on an unseen test set. A node is removed only if the decision tree’s accuracy is decreased or remains the same by removing it. Pruning is done iteratively by considering each node and stops when the accuracy starts getting worse.    


+ <span style="color:blue">Describe one problem with using information gain as a statistical measure?</span>   
Information gain favors those attributes that can take many values e.g. “Date” over attributes that can take few values e.g. “Gender”. This is the natural bias of information gain. This can be avoided by using other measures like gain ratio which uses split information.    


**Topics for further exploration**:
1. Version-space (set of short-listed hypotheses that satisfy the training data)  
2. Candidate-elimination algorithm (it lets all the hypotheses in the version space vote for the new test instance)  
3. Rote-learner algorithm (simply stores the training data in memeory and compares test data to it for classification deductively using no inductive bias.)  
4. Find-S (finds the most specific hypothesis to classify later instances)  


## Thought of the Week:  
This week's picks from articles on COVID-19:  
1. The Shift Americans Must Make to Fight the Coronavirus by Meghan O'Rourke. [4]    
*"Americans today are not used to the notion that an infectious disease we have no treatment for might sweep through the nation. It startles the mind that our high-tech, modern medical system, which routinely wrests sick people from the arms of death, could actually be overwhelmed by a challenging but not remarkably deadly virus. One hundred or so years ago, most deaths in the U.S. were caused by infectious disease. Today, most are caused by chronic conditions. This is new to us...But we also live in a country stubbornly hung up on a damaging idea of self-reliance, a nation pathologically invested in the idea that we should all “just do it”—an attitude that challenges us to muscle through it—whatever it might be. We have no shared discourse for the idea that the hard thing to do, the truly challenging thing to do, might be to do less in order to help another...We are so addicted to the concept of individual responsibility that we have a fragmented health-care system, a weak social safety net, and a culture of averting our eyes from other people’s physical vulnerability...Unfortunately, there is no prescription for the “right” thing to do, no answer to the question of whether to have the playdate or fly to Florida right now. But there is an ethos to cling to...To be ill is to know our interconnectedness, but to be ill in America today is to be brought up against the pathology of a culture that denies this fact...No person is an island; the nation that believes in individuals more than it values community risks its own survival. Accepting and embracing this might be the tougher path than muscling through, each by each, whatever comes."*  

## References:
[1] [Machine Learning by Tom M. Mitchell](http://www.cs.cmu.edu/~tom/mlbook.html)    
[2] [Tree algorithms: ID3, C4.5, C5.0 and CART](https://medium.com/datadriveninvestor/tree-algorithms-id3-c4-5-c5-0-and-cart-413387342164)  
[3] [C4.5: Programs for Machine Learning](https://books.google.com/books?hl=en&lr=&id=b3ujBQAAQBAJ&oi=fnd&pg=PP1&dq=+Quinlan,+J.+R.+C4.5:+Programs+for+Machine+Learning&ots=sQ9qZKBrE6&sig=-NBb9etYKsJ0lbxmoJ19zGbWztE#v=onepage&q=Quinlan%2C%20J.%20R.%20C4.5%3A%20Programs%20for%20Machine%20Learning&f=false)   
[4] [In simple language, how does C4.5 deal with missing values?](https://www.quora.com/In-simple-language-how-does-C4-5-deal-with-missing-values)  
[5] [The Shift Americans Must Make to Fight the Coronavirus](https://www.theatlantic.com/ideas/archive/2020/03/we-need-isolate-ourselves-during-coronavirus-outbreak/607840/?utm_campaign=the-atlantic&utm_medium=social&utm_source=facebook&fbclid=IwAR05XObEWpe58C8eJy9YR9cOoH7rrb5yeRO8hKT93AnuT-bLpNxZhLOcOjs)  
