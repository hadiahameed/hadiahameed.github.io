---
layout: post
title: "Episode 9: ML Notes (Part II): Fundamentals of Decision Tree Theory"
description: Notes from Tom M. Mitchell's book
tags: [machinelearning, theory, datascience, decisiontrees]
date: 2020-03-20
---

These days I am reading the book <a href="http://www.cs.cmu.edu/~tom/mlbook.html">"Machine Learning"</a> by Tom M. Mitchell and I will be documenting some important concepts from its chapters in a series of blog posts.

## [Background/Important Terminology]: 
Decision trees iteratively partition the data and generate rules which helps us to make necessary predictions. 
<figure>
    <img src="https://i0.wp.com/www.samtalksml.net/wp-content/uploads/2017/05/image_dt1-1.png?resize=450%2C368&ssl=1" width="75%" height="75%">
    <figcaption>Source: <a href="https://www.samtalksml.net/helping-doctors-validate-decision-trees/">Helping doctors validate decision trees by Samuel Laurence</a></figcaption>
</figure> 

## Concepts:  
+ <span style="color:blue">What are the different decision tree algorithms and how do they differ from each other?</span>  
**Iterative Dichotomiser 3 (ID3):**  
    - Developed by Ross Quinlan 1986  
    - Starts with an empty tree. Iteratively constructs a final tree that’s supposedly the shortest tree that is consistent with the training data  
    - Uses post-pruning to fix overfitting  
    - Uses information gain to select attributes  
    - Works only with discrete-valued attributes    

**C4.5:**   
    - Successor of ID3  
    - Works with both discrete and continuous-valued attributes (by partitioning)  
    - Converts ID3 into if-then rules  
    - Has a special technique for replacing missing values. If an attribute is missing, it assigns probability to each possible value of that attribute  

**CART:**   
    - Similar to C4.5 but in addition to numerical-valued attributes, it can also support numerical target values (regression) instead of just discrete-valued targets   
    - Creates binary trees  

+ <span style="color:blue">What are the different ways in which entropy can be understood?</span>  
    - ID3 decision tree algorithm works by finding the best attribute to use at each node of the tree.
    - It selects attribute with the highest information gain which is a statistical measure.
    - Information gain uses entropy which is a measure used to characterize the impurity of a set of examples. 
    - Entropy of 0 means all the examples in the set belong to the same target class. (pure data. Impurity is 0)
    - Entropy of 1 means half the examples belong to one class and the other half belong to the other class. (very impure data)
    - It is the minimum number of bits needed to encode information about the target class of a given instance.
    - Information gain of an attribute measures the drop in entropy (impurity) if the data was split using this attribute. It tells us the number of bits saved in encoding the information of the target class, if this attribute was used to split the data. 

+ <span style="color:blue">What is inductive learning?</span>  
Inductive learning algorithms guarantee that the target concept will fit the training data perfectly. The core assumption in inductive learning is that the unseen data will follow the distribution of the training data so our learned hypothesis will fit the unseen data too. Formally, the inductive learning hypothesis says:  
1. Any hypothesis found to approximate the target concept well over a sufficiently large set of training examples will also approximate the target concept well over other unobserved examples.  
2. Inductive learning makes certain assumptions called **inductive bias** that help it classify unseen instances. For example, an inductive learner might assume that the output class can only be expressed as a conjunction (AND gate) of input variables:  
(e.g. If Temperature = High AND Wind = Weak AND Humidity = Low then class is positive).  
The leaner is biased in that it does not allow any disjunctions (e.g. Temperature = High OR Low) here.  
3. Usually inductive learners are compared based on the strength or weakness of the inductive biases they make.  
Decision Trees are an example of inductive learning, which allow disjunction of conjunctions.  
(e.g. If (Temperature = Low AND Wind = Weak) OR (Humidity = High AND Wind = Weak), Play_Tennis = Yes)). 

+ <span style="color:blue">What is meant by syntactically and semantically distinct hypotheses?</span>  
Consider a feature space X = [Temperature, Wind, Humidity]. Temperature can take 3 distinct values, whereas Wind and Humidity each take 2 distinct values. Then there can be **3 * 2 * 2 = 12** distinct instances with all the possible combinations of the three features.  
In this case, a hypothesis refers to a function that we try to learn in order to decide whether one should play outside or not, based on the values of X.  For example:  
(**\<High,Weak,Low\>,1**) means when the temperature is high, wind is weak and humidity is low, one can play outside (Target class = 1).  
Similarly (**\<?,Strong,?\>, 0**) implies that I don’t care what the temperature or humidity is, as long as there is strong wind, I won’t play outside (target class = 0). “?” here is called a wild card.  
Moreover, (**<&Phi;, &Phi;, &Phi;,1>**) means I have no information about the features (empty instances) so I will play outside (target class = 1.   
This means that each feature can take the following values  
Temperature (3 + 2 = 5): High, Low, Medium, ?, &Phi;  
Wind (2 + 2 = 4): Strong, Weak, ?, &Phi;  
Humidity ( 2 + 2 = 4): Low, High, ?, &Phi;  
So, the number of **syntactically distinct hypotheses** is **5 * 4 * 4 = 80**  
But if there is even a single &Phi; in the input vector, it means that values for all the other features are also unknown. In other words, if we do not know what temperature it is, we also do not know about humidity and wind. So there is a single instance where the value for all three features is null i.e. (**<&Phi;, &Phi;, &Phi;>**). So, the number of **semantically distinct hypotheses** is **(4 * 3 * 3) + 1 = 37**  
This is the entire hypotheses space that we will need to search in order to learn a concept and use machine learning to decide whether we should play outside or not.   

+ <span style="color:blue">How do you interpret the following expressions:</span>

1. **C : X &rarr; {0,1}**  
There is a target concept **C** that we aim to learn. **X** is the set of instances over which **C** is defined. The target concept **C** maps the set of instances **X** to a set of boolean values 0 and 1.  
Example: Predicting whether to play outside or not based a set of features X = Temperature, Wind, Humidity.
In this case, C(X) = 1 means play outside. C(X) = 0 means don't play outside. 
2. **(&forall; x &epsilon; X)[(h2 (x) = 1) &rarr; (h1(x) = 1)]**
This is called **more_general_than_or_equal_to rule**. It means for any instance x in X, if it satisfies hypothesis h2, it also satisfies hypothesis h1, which implies that *h1 is more general than h2*. For example:  
H1 = <High, ? ,?> Temperature is high and I don’t care about other variables.  
H2 = <High,Weak,?> Temperature is high, wind is weak and I don’t care about humidity. 
H1 is more general than H2 as it will classify all instances with high wind as class = 1, where h2 is more specific and requires wind to be weak as well as temperature to be high for the output class to be 1.  
3. <img src="{{site.baseurl}}/assets/induction.png" width="30%" height="30%">  
D is the training data. x is a new instance from the test set. L is a learner. The expression means that L inductively infers from the training data and test instance x that the class of x is L(x,D) = 1 (positive).  
3. <img src="{{site.baseurl}}/assets/deduction.png" width="30%" height="30%">  
B is the inductive bias of the learner L. This expression means that the learner “deduces” the class for test instance x, using bias B, training data D and description of test instance x. 


+ <span style="color:blue">How is more_general_than_or_equal_to rule applied in concept learning?</span>   
Concept learning is about finding the best hypothesis that satisfies a given training set. This partial ordering rule is useful because if we have a very large hypotheses space to search, we can order them according to their generality and test only a handful of them, thus exploiting the structure of the hypothesis space instead of exhaustively trying and testing each and every hypothesis. You start with the most specific hypothesis and generalize it each time it fails to satisfy the training set. We want to find the maximally specific hypothesis (that is general enough to perfectly fit the training data).  
The most specific hypothesis is **<&Phi;, &Phi;, &Phi;>** (you don’t classify any instance as class = 1)  
The most general hypothesis is **\<?,?,?\>** (no matter what the values of the features, you label them as class = 1)  

+ <span style="color:blue">What is the difference between a hypothesis being **consistent** with a training example and a training example **satisfying** a hypothesis?</span>   
H is consistent with training example x if:  
x belongs to class 1 and H also labels it as 1 i.e. (H(x) = C(x) where C is the actual target concept and H is the learned hypothesis)  
x satisfies H if:  
No matter what class x belongs to in reality, H labels it as 1 i.e. (h(x) = 1)

**Topics for further exploration**:
1. Version-space (set of short-listed hypotheses that satisfy the training data)  
2. Candidate-elimination algorithm (it lets all the hypotheses in the version space vote for the new test instance)  
3. Rote-learner algorithm (simply stores the training data in memeory and compares test data to it for classification deductively using no inductive bias.)  
4. Find-S (finds the most specific hypothesis to classify later instances)  


## Thought of the Week:  
This week's picks from articles on COVID-19:  
1. The Shift Americans Must Make to Fight the Coronavirus by Meghan O'Rourke. [2]    
*"Americans today are not used to the notion that an infectious disease we have no treatment for might sweep through the nation. It startles the mind that our high-tech, modern medical system, which routinely wrests sick people from the arms of death, could actually be overwhelmed by a challenging but not remarkably deadly virus. One hundred or so years ago, most deaths in the U.S. were caused by infectious disease. Today, most are caused by chronic conditions. This is new to us...But we also live in a country stubbornly hung up on a damaging idea of self-reliance, a nation pathologically invested in the idea that we should all “just do it”—an attitude that challenges us to muscle through it—whatever it might be. We have no shared discourse for the idea that the hard thing to do, the truly challenging thing to do, might be to do less in order to help another...We are so addicted to the concept of individual responsibility that we have a fragmented health-care system, a weak social safety net, and a culture of averting our eyes from other people’s physical vulnerability...Unfortunately, there is no prescription for the “right” thing to do, no answer to the question of whether to have the playdate or fly to Florida right now. But there is an ethos to cling to...To be ill is to know our interconnectedness, but to be ill in America today is to be brought up against the pathology of a culture that denies this fact...No person is an island; the nation that believes in individuals more than it values community risks its own survival. Accepting and embracing this might be the tougher path than muscling through, each by each, whatever comes."*
2. Analysis of the genetic make-up also revealed a never-before-seen feature in this family of coronavirus: sticky sugar molecules that may allow it to hide itself from attacks by the human immune system...Until Beijing banned the practice some weeks later, China allowed licensed captive breeding for commercial purposes and there was no restriction on consumption of wildlife...But another consequence of China’s trade in wildlife for the dining table is animals that don’t normally live together in the wild are thrown together in cages, often in confined, unsanitary conditions.
3. Watch the movie Contagion.  
4. Read how Singapore, Taiwan and Hong Kong are beating the pandemic [2].  
5. Read about three researchers in Canada who brought the world closer to developing a vaccine for COVID-19 [3].  
6. Wash your hands. 

## References:
[1] [Machine Learning by Tom M. Mitchell](http://www.cs.cmu.edu/~tom/mlbook.html)  
[2] [The Shift Americans Must Make to Fight the Coronavirus](https://www.theatlantic.com/ideas/archive/2020/03/we-need-isolate-ourselves-during-coronavirus-outbreak/607840/?utm_campaign=the-atlantic&utm_medium=social&utm_source=facebook&fbclid=IwAR05XObEWpe58C8eJy9YR9cOoH7rrb5yeRO8hKT93AnuT-bLpNxZhLOcOjs)  
[3] [Canadian Researchers Have Isolated the Novel Coronavirus](https://www.vice.com/en_ca/article/pkevm7/coronavirus-update-sunnybrook-researchers-isolated-covid-19?utm_source=vicenewsfacebook&fbclid=IwAR27wwBNUrByCx7qr4OGJWPzY5XCVh7_6dHK1Ac2d_Ii_ZQW-eIccg0zLS0)  
