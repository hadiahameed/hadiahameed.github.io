---
layout: post
title: "Episode 5: Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville"
description: Notes on linear algebra and probability
tags: [statistics, probability, linearalgebra, mathematics, deeplearning]
date: 2019-10-10
---

This week I read the first few chapters of the book, <a href="http://www.deeplearningbook.org"> "Deep Learning" [1]</a> by Ian Goodfellow, Yoshua Bengio and Aaron Courville, and this is everything I want to remember from it. (The book is available online [1] for free!)

[Background: Ian Goodfellow is famously known as the father of Generative Adversarial Networks (GANs) which he invented in 2014 for data augmentation. Earlier this year, he left Google and started working for Apple [3]. Later on, GANS became controversial as they were being used in "deepfake" face-swapping technology [3] and false news and media generation such as the fake video of Donald Trump addressing the people of Belgium on climate change [2]. There is an actual website [4] called thispersondoesnotexist.com and has photos of non-existent people, generated using GANs.]  

## Concepts:  
+ <span style="color:blue">What is a singular matrix?</span>  
A singular matrix has:
    - linearly dependent columns.  
    - all its eigenvalues equal to zero.  
    - determinant equal to zero.  
    - no inverse.


+ <span style="color:blue">When is it appropriate to use L1-norm?</span>  
L2 norm increases very slowly near the origin. It is not suitable for comparing values close to zero. So when it is important to discriminate between 0 and very small non-zero values, it is desirable to use L1 norm.


+ <span style="color:blue">What is an orthogonal matrix?</span>  
In an othrogonal matrix, every column is:
    - othrongonal to each other (inner product is zero)
    - every column is orthonormal (norm is 1)   
If A is an orthogonal matrix, it implies A<sup>-1</sup>= A<sup>T</sup>, since A<sup>T</sup>A = AA<sup>T</sup>  


+ <span style="color:blue">What is an eigenvector?</span>  
Consider a vector **v** with *n* elements.  If you multiply it with a square matrix A, where A is *n*x*n*-dimensional, you will get a new *n*x1-dimensional vector **v'**.   

    Now if **v'** is simply a scaled version of **v** i.e. **v'** = **&lambda; v**, then **v** is called the eigenvector of A and **&lambda;** are eigenvalues of A. Even if you use some scaled version of **v**, say **sv** where **s &ne; 0**, eigenvalues of A remain the same.


+ <span style="color:blue">When is a matrix positive semi-definite?</span>  
Matrix with positive eigenvalues is positive definite.  
Matrix with positive or zero eigenvalues is positive semidefinite. **&ForAll; x , x<sup>T</sup>Ax &ge; 0**


+ <span style="color:blue">What is eigenvalue decomposition?</span>  
A = VDV<sup>-1</sup>  
Where:  
V = concatenate all eigenvectors of A in one matrix V. It will be an orthogonal matrix.  
D = Put all eigenvalues of A in a diagonal matrix D   

    *A needs to be positive semidefinite in order to be able to decomposed this way.*

<figure>
    <img src="https://www.sharetechnote.com/image/EngMath_Matrix_EigenDecomposition_02.png">
    <figcaption>Source: <a href="https://www.sharetechnote.com/html/Handbook_EngMath_Matrix_EigenDecomposition.html">Share Tech Note</a></figcaption>
</figure>   


+ <span style="color:blue">What is singular value decomposition?</span>   
Unlike eigenvalue decomposition, every matrix has SVD.  
A = UDV<sup>T</sup>  
Where:  
U = Contains left singular vectors  of A. These are equal to the eigenvectors of AA<sup>T</sup> [5]  
V = Contains right singular vectors  of A. These are equal to the eigenvectors of A<sup>T</sup>A [5]  
D = Diagonal matrix that contains singular values of A. These are equal to the square roots of eigenvalues from AA<sup>T</sup> or A<sup>T</sup>A. [5]  

<figure>
    <img src="https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/229f77d2cb173c1cef4d6cfbab2e905e/svd_matrices.jpg">
    <figcaption>Source: <a href= "https://blogs.oracle.com/r/using-svd-for-dimensionality-reduction">Blogs.Oracle</a></figcaption>
</figure>   


+ <span style="color:blue">What is the difference between Probability Theory and Information Theory?</span>   
Probability theory allows us to make uncertain statements and to reason in the presence of uncertainty.   
Information theory enables us to quantify the amount of uncertainty in a probability distribution.  


+ <span style="color:blue">What is the difference between Frequentist and Bayesian Probability?</span>   
The kind of probability related to the rates at which events occur (drawing cards, tossing coins) is called frequentist probability.  
The one that is related to qualitative levels of certainty (disease diagnosis) is known as Bayesian probability.  


+ <span style="color:blue">What is so special about the delta function?</span>   
Dirac delta function is defined such that it is zero valued everywhere except 0, yet integrates to 1. It is a special kind of function called **generalized function** that is defined in terms of its properties when integrated.   

<figure>
    <img src="https://i.ytimg.com/vi/gp19DWH0m-0/maxresdefault.jpg" width="300" height="200">
    <figcaption>Source: <a href="https://www.youtube.com/watch?v=gp19DWH0m-0">WikiAudio</a></figcaption>
</figure>   

## Thought of the Week:  
I found this really useful image shared by Dr. Zeeshan-ul-hassan Usmani on his LinkedIn [6] a couple of weeks ago. He is originally from Pakistan and currently works as a Data Science, AI & Blockchain expert in Claifornia. I think the following image is useful in learning how to go about machine learning in a sensible and systematic way instead of going down the rabbit hole, shopping for classifiers and never returning!  

<img src="{{site.baseurl}}/assets/cheatsheet.jpg">
  
See you next week!

## References:
[1] [Deep Learning](http://www.deeplearningbook.org)  
[2] [You thought fake news was bad? Deep fakes are where truth goes to die](https://www.theguardian.com/technology/2018/nov/12/deep-fakes-fake-news-truth)  
[3] [Father of GANs Ian Goodfellow Splits Google For Apple](https://www.pbs.org/kenburns/civil-war/war/historical-documents/sullivan-ballou-letter/)  
[4] [Fake image website](thispersondoesnotexist.com)  
[5] [Singular Value Decomposition (SVD) tutorial](http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm)  
[6] [Dr. Zeeshan-ul-hassan Usmani, LinkedIn](https://www.linkedin.com/in/zusmani/)


