---
layout: post
title: "Episode 11: Notes from Machine Learning Specialization by Andrew Ng"
description: Taught by Andre Ng, DeepLearning.AI
tags: [machinelearning, coursera]
date: 2023-03-27

---

This is part of a series of posts on some of my notes from course on Machine Learning Specialization taught by Prof. Andrew Ng[1].

## Concepts:  

+ <span style="color:blue">For anomaly detection, in what cases do we prefer supervised learning and when do we use unsupervised learning?</span>       

  In anomaly detection, we are dealing with an imbalanced dataset. 

  - **Unsupervised learning:**
      - when we have a very small number of positive examples (0 - 20 is common) and a large number of negative examples. Supervised learning: Otherwise.
      - Anomalous behavior has no discernible pattern. Each time the anomaly can occur in a different way. Unsupervised learning methods e.g. density estimation are independent of this as they try to model the behavior of normally occurring phenomenon and everything outside it is considered as an anomaly. 
      - Choice of features for anomaly detection is extremely important.

  - **Supervised learning:**
      - 20 or more positive examples.
      - Here we try to learn the behavior of anomalous examples. Based on the assumption that future anomalous examples will look like the existing ones. 
      - Choosing redundant or extra features doesn't cause much harm as the algorithm takes care of it. 

  Examples:
  - Fraud detection in banks, monitoring machines in data center (Unsupervised)
  - Email spam, disease classfication, weather prediction (Supervised) 


  + <span style="color:blue">What are some of the ways you can convert non-gaussian features to gaussian and why is it important?</span>  
  If we are estimating the probability distribution of the negative (non-anomalous) examples using a gaussian distribution that it is important to convert all the features to a gaussian distribution. 

  If x is some feature with non-gaussian distribution then we can convert it to gaussian through the following transformations:

  $$ log(x)\\
  log(x + c) \\ 
  log(x + c) $$ where c is some constant value selected by hit and trial. We add this to avoid log(0) case.\\  
  $$ \sqrt(x)\\
  x^(1/3) $$


   <figure>
    <img src="{{site.baseurl}}/assets/gaussian.png" width="55%" height="55%">       
    <figcaption>Source: <a href="https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/7MOXj/choosing-what-features-to-use"> Week 1, Unsupervised Learning, Recommenders, Reinforcement Learning by Andrew Ng</a></figcaption>
   </figure> 

+ <span style="color:blue">What do we do when after building our density estimation model we find out that the probability of an anomalous event is close to a normal event?</span>   

   In these cases, it helps to come up with a derived variables e.g. ratio of a feature that takes a very small value for anomalies and a feature that take very high value in case of anomalies. 

   <figure>
    <img src="{{site.baseurl}}/assets/problem_anomaly.png" width="55%" height="55%">       
    <figcaption>Source: <a href="https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/7MOXj/choosing-what-features-to-use"> Week 1, Unsupervised Learning, Recommenders, Reinforcement Learning by Andrew Ng</a></figcaption>
   </figure> 

+ <span style="color:blue">Why do we add non-linear activations in neural nets?</span>   
   A linear layer is simply a linear combination of a set of weighted inputs. If we only have linear layers, all these layers can be squished into a single layer because combination of linear layers gives another linear layer (linear combination properties). There is no new information being added to the system if we only have multiple linear layers in the network.   

+ <span style="color:blue">What kind of transformation is matrix multiplication? Is translation a linear transformation</span>  
   Matrix multiplication is a linear transformation. All the following transformations can be performed by simple matrix multiplication:  
   - Rotation (orthonormal matrix)
   - Reflection (negative determinant)
   - Scaling/Stretching
   - Shearing  

    Translation is not a linear transformation as it cannot be expressed as a product of a matrix and a vector because it involves adding scalars and 0 is not mapped to 0 after this translation. 
â€‹	

+ <span style="color:blue">What is chain rule?</span>   
   If we have two functions C and G and we want to take their derivative w.r.t some variable w, then chain rule says that the derivative of C(G(w)) is equal to derivative of function C at point G(w), multiplied by derivative of G with respect to w. Neat, innit! 
   <a href="https://www.codecogs.com/eqnedit.php?latex=C(G(w))'&space;=&space;C'(G(w))&space;\cdot&space;G'(w)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C(G(w))'&space;=&space;C'(G(w))&space;\cdot&space;G'(w)" title="C(G(w))' = C'(G(w)) \cdot G'(w)" /></a>  

+ <span style="color:blue">What happens when we take the derivative of a scalar S with respect to a vector V? What is a Jacobian matrix?</span>  
  When we take derivative of a scalar (e.g. cost function) with respect to (w.r.t.) a vector (e.g. input/state vector), we get a new vector of gradients the same size as vector V.  
  If we take the derivative of a vector **V**<sub>1</sub> with dimension px1, w.r.t a vector **V**<sub>2</sub> with dimension qx1, we get a matrix of size pxq (Jacobian matrix)  
  
  <figure>
    <img src="{{site.baseurl}}/assets/lecun_i14.png" width="55%" height="55%">       
    <figcaption>Source: Here c is a scalar cost function and z<sub>g</sub> and z<sub>f</sub> are vectors [1]. </figcaption>
  </figure> 

## Thought of the Week:  

Generative Pre-trained Transformer 3 (GPT-3) is here! It is now the largest language model that we know of, developed by an independent lab called OpenAI in San Francisco. It has 175 billion parameters and has been trained on millions of text documents including Google books and Wikipedia articles. The underlying principle behind GPT modeling is semi-supervised learning which means to train the model on a large-sized unlabeled data in unsupervised manner and then fine-tune it by supervised learning on small set of labeled examples. [4]  

Even though GPT-3 was developed for sentence completion tasks by its developers but it turns out that it can also be used to write code! Seems like it's time for us computer scientists to get insecure about our jobs. Check out this interesting article in <a href="https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html"> New York Times</a> to know more fun stuff about GPT-3. 

 Until next time! 

## References:

[1] [Yann LeCun Course](https://atcold.github.io/pytorch-Deep-Learning/)  
[2] [0th order methods](http://adl.stanford.edu/aa222/Lecture_Notes_files/chapter6_gradfree.pdf)  
[3] [Kaiming Trick of weight initialization](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)  
[4] [GPT-3](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2)  