<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

	<title>Hadia Hameed</title>
	<link href="http://hadiahameed.github.io/blog/atom.xml" rel="self"/>
	<link href="http://hadiahameed.github.io/blog"/>
	<updated>2019-09-26T10:44:11-04:00</updated>
	<id>http://hadiahameed.github.io/blog</id>
	<author>
		<name>Hadia Hameed</name>
		<email>hadiahameeduet@gmail.com</email>
	</author>

	
		<entry>
			<title>Episode 4: Book Review: Dataclysm: Who We Are (When We Think No One's Looking) by Christian Rudder</title>
			<link href="http://hadiahameed.github.io/blog/2019/09/26/episode4"/>
			<updated>2019-09-26T00:00:00-04:00</updated>
			<id>http://hadiahameed.github.io/blog/2019/09/26/episode4</id>
			<content type="html">&lt;p&gt;This week I read the book, &lt;a href=&quot;https://www.amazon.com/Dataclysm-When-Think-Ones-Looking/dp/B00M284HDO&quot;&gt; “Dataclysm - Who We Are (When We Think No One’s Looking)” [1]&lt;/a&gt; by Christian Rudder, and this is everything I want to remember from it.&lt;/p&gt;

&lt;p&gt;[Background: Christian Rudder is the co-founder of the online dating site OkCupid. He got himself into a mess when he said he used data from his site to experiment on human beings. Earlier, he started a blog [2] to share his reflections from the data collected by OkCupid, which later turned into a book [1]. Rudder grew up in Arkansas, and like other co-founders of OkCupid, he was a math major at Harvard. In his book he explains some useful mathematical and data science concepts, using highly inappropriate examples from the world of online dating.]&lt;/p&gt;

&lt;h2 id=&quot;concepts&quot;&gt;Concepts:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:blue&quot;&gt;What is variance?&lt;/span&gt;&lt;br /&gt;
The author explains variance with the help of an unusual example, one that you would probably not use in a classroom, but you can always replace it with a more appropriate scenario that uses the same underlying theory.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The graph below shows how men rated women based on their attractiveness, with 1 and 5 being the score assigned to the least and most attractive women, respectively. &lt;br /&gt;
&lt;img src=&quot;/assets/women_score.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Take the group of women who get the average score of 3.0. The author lists five scenarios describing how men would have voted. The vote patterns become more polarized from A to E, but they all average out to give 3.0 as the final score. Variance is “how widely data is scattered around a central value. It goes up the further the data points fall from the average.” Variance is often used by investors to decide between two companies that return the same amount of profits in the same period of time. The one that has lower variance (and hence, “fewer heart palpitations”) is considered to be a more stable investment.&lt;br /&gt;
&lt;img src=&quot;/assets/vote_patterns.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What is the famous “Ballou Letter”?&lt;/span&gt;&lt;br /&gt;
Major Sullivan Ballou fought on the Potomac under the Union Army in mid-19th century. He wrote a heartfelt farewell letter to his wife a week before he died. The letter was never mailed and was recovered from the battlefield by the Governor of Rhode Island when he visited Virginia. There is also a movie “The Civil War” by Ken Burns, in which the letter is read out loud by one of the characters. The author writes that, the Ballou letter has become so famous that when it searched “famous letter” on Google, it shows up second in the list (though Google doesn’t do the same for me!) [3]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What is culturomics?&lt;/span&gt;&lt;br /&gt;
It’s the study of tracking changes in the usage of words through time. The term was first used by the researchers Jean Baptiste Michel and Erez Lieberman Aiden from Harvard and other researchers from MIT, in their paper “Quantitative Analysis of Culture Using Millions of Digitized books.” [4] Rudder writes, &lt;em&gt;“Once language and data come together, it’s that extra dimension, time, that’s so compelling…Google books is working to repair our historical blind spot: in collaboration with libraries around the world, they have digitized 30 million unique books, great and small, and, true to their expertise, they have made the whole searchable. The body of data has created a new field of &lt;strong&gt;quantitative cultural studies&lt;/strong&gt; called &lt;strong&gt;culturomics&lt;/strong&gt;.”&lt;/em&gt; The author show a chart that he calls “Pizza Now, Pizza Forever”&lt;br /&gt;
&lt;img src=&quot;/assets/word_use.jpg&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;How have smartphones affected the average message length?&lt;/span&gt;&lt;br /&gt;
This one in particular made me terribly sad. The data from OkCupid shows that the best messages that get the highest response rate have decreased in length drastically over the years. Nowadays, a good message is considered to be only 40-60 letters long. I already feel misplaced in this time and age, as I am used to sending painstakingly long, hand-written letters and emails to my friends. Seeing these figures makes me wonder if they even read my letters at all.&lt;br /&gt;
&lt;img src=&quot;/assets/app_store.jpg&quot; /&gt;
&lt;img src=&quot;/assets/word_length.jpg&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;Favorite quote from the book:&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;“…Writing, like life itself, abides. It changes form, it replicates in odd ways, it finds unexpected niches…it even, like anything alive, occasionaly stinks. But realize this: we are living through writing’s Cambrian explosion, not its mass extinction. Language is more varied than ever before, even if some of it is directly copied from the clipboard – variety is the preservation of an art, not a threat to it. From the high-flown language of literary fiction to the simple, even misspelled, status update, through all this writing runs a common purpose. Whether a friend to friend, stranger to stranger, lober to lover, or author to reader, we use words to connect. And as long as there is a person bored, excited, enraged, transported, in love, curious, or missing his home and afraid for his future, he’ll be writing about it.”&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;thought-of-the-week&quot;&gt;Thought of the Week:&lt;/h2&gt;
&lt;p&gt;This is how the past few days have looked like and its getting busier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/giphy.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But one thing that helps me in dealing with all that chaos in my life is imagining myself next to an assembly line, with tasks, big and small, simply whizzing by and all I need to do is simply lift them one or two at a time and get the work done. There are, of course, a number of caveats to this type of sunny-side-up way of thinking and it’s useful to be mindeful of them:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All the time you spend worrying about how little time you have, is all the time you waste not doing the work.&lt;/li&gt;
  &lt;li&gt;Often the only thing that’s stopping us from working is not knowing where to start. That is what stops us from getting up early every day and charging into the day to get the actual work done.&lt;/li&gt;
  &lt;li&gt;We counter our anxieties by scrolling through our news feeds, binge-watching TV shows and feeling guilty afterwards, staying up late for no good reason, and then running amok, announcing to the world how little time we have. We all do it, no one’s immune to it but what eventually saves us is having an ideal baseline that we can return to, one that allows us to live up to our highest potential. It’s very important to have that ideal baseline.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See you next week!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://www.amazon.com/Dataclysm-When-Think-Ones-Looking/dp/B00M284HDO&quot;&gt;Dataclysm&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&quot;https://theblog.okcupid.com/&quot;&gt;OkCupid Blog&lt;/a&gt;&lt;br /&gt;
[3] &lt;a href=&quot;https://www.pbs.org/kenburns/civil-war/war/historical-documents/sullivan-ballou-letter/&quot;&gt;Sullivan Ballou Letter&lt;/a&gt;&lt;br /&gt;
[4] &lt;a href=&quot;https://www.researchgate.net/publication/  49688894_Quantitative_Analysis_of_Culture_Using_Millions_of_Digitized_Books&quot;&gt;Culturomics&lt;/a&gt;&lt;/p&gt;

</content>
		</entry>
	
		<entry>
			<title>Episode 3: People are Not Guinea Pigs</title>
			<link href="http://hadiahameed.github.io/blog/2019/09/19/episode3"/>
			<updated>2019-09-19T00:00:00-04:00</updated>
			<id>http://hadiahameed.github.io/blog/2019/09/19/episode3</id>
			<content type="html">&lt;p&gt;This week I read an article, &lt;a href=&quot;http://openingpathways.org/people-are-not-problems&quot;&gt; “People are not problems” [1]&lt;/a&gt; by Erik Johnston and Jessica Givens, and it took me on a reading spree as I hopped from one hyperlink to another.&lt;/p&gt;

&lt;p&gt;[Background: Opening pathways is an open-source project around healthcare, led by patients and researchers where the Principal Investigator is a patient. They have an on-call data science team that provides technical support to people who are interested in scientific inquiry and research, especially in the field of diabetes.]&lt;/p&gt;

&lt;h2 id=&quot;concepts&quot;&gt;Concepts:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:blue&quot;&gt;What is citizen science?&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When a group of people from the general public are interested in solving their daily problems using basic science, they form a citizen-science community [2]. Examples:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;People in Flint, Michigan sampled and tested water from their homes and neighborhoods and reported it as scientific data. [3]&lt;/li&gt;
  &lt;li&gt;Nightscout is an open-source, DIY project where people can monitor and analyze their glucose levels in real-time by uploading data from body-worn sensors such as continuous glucose monitors (CGM). [4]&lt;/li&gt;
  &lt;li&gt;Do-It-Yourself gene-editing kits that allow hobbyists to perform techniques like CRISPR and SLiCE. [2]&lt;/li&gt;
  &lt;li&gt;Community labs like Genspace in Brooklyn, NYC where people can come and work on their projects, using the equipment and supplies for a small membership fee. [6]&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:blue&quot;&gt;What is architecture of participation&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a term that was first used by Tim O’Reilly, who is the founder of an online learning platform called O’Reilly Media, to describe the working of Web2.0. Architecture of participation implies “&lt;em&gt;joint creation and production&lt;/em&gt;”, creating a culture of open-source projects and contributions. It is pretty much the basis of Web2.0 where a “&lt;em&gt;community of users contributes to the content or the design and development process.&lt;/em&gt;” It essentially overhauls the traditional system of “&lt;em&gt;command and control&lt;/em&gt;”.&lt;/p&gt;

&lt;p&gt;Marten Mickos, who is the head of the Cloud unit of Hewlett Packard writes in his article “&lt;em&gt;An architecture of participation&lt;/em&gt;” [7].&lt;/p&gt;

&lt;p&gt;“&lt;em&gt;The architecture of participation is more than open, and more than crowd-sourcing. Open, strictly speaking, means that you share your production with others. It doesn’t necessarily mean participation. Crowdsourcing means many people contribute to a production. It doesn’t necessarily mean that they would exchange value with each other. It’s not enough to be open and it’s not enough to crowd-source. We must build an architecture of participation where different participants with different agendas can exchange ideas and models, and everyone has access to the end results… That’s the essence of the architecture of participation. You construct rules of engagement that allow disagreeing people to let their work products agree. This is a system where the designer invites input from contributors. The end result is an ecosystem that evolves faster than any individual initiative, resulting in a work product with fewer deficiencies.&lt;/em&gt;”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What about free-riders in a culture of open-source projects?&lt;/span&gt;&lt;br /&gt;
Marten Mickos writes, “&lt;em&gt;It does not matter whether there are free riders or freeloaders in the system, because the moment they take any action whatsoever, they become at least marginally useful to the entire system. Millions of freeloaders providing a marginal benefit amounts to much more than a small number of contributors each providing a big benefit. This is why the size of the ecosystem matters.&lt;/em&gt;”
” [7]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What are some examples of open-architecture systems/services?&lt;/span&gt;&lt;br /&gt;
Wikipedia, Facebook, Khan Academy, Twitter, oDesk, Mechanical Turk, The Human Genome Project, The Linux Foundation, Kiva.org.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;How can academic research incorporate architecture of participation and citizen science?&lt;/span&gt;&lt;br /&gt;
Academic research has long been considered a breeding ground for intellectual snobbery, where researchers often regard the people they are trying to help as mere “subjects” or “data samples”. Therefore, many people feel disillusioned with the findings and studies that seem removed from their daily struggles, and rightly so. I have always found the idea of using people merely as “subjects” for proving/disproving a particular hypothesis, extremely problematic. It is wrong for researchers to think that by conducting studies and publishing papers in top-notch conferences and journals, they are somehow handing out viable solutions to the “illiterate target communities” from their “ivory towers of intellectual inquiry”. It neither empowers the people nor builds their trust in scientific solutions, in fact it dehumanizes them and only adds to the existing pool of scientific publications relevant and accessible to the privileged few.  Erik Johnston, who is the director of Policy Informatics at the Decision Theatre at Arizona State University writes, “&lt;em&gt;we must reframe communities from “passive problem generators only to be included when the solution has been created from the ivory tower and needs testing” to viewing communities as driven, curious people who have lived experienced and expert knowledge on a challenge and are eager to be part of the discovery process and take ownership of the solutions.&lt;/em&gt;”&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;thought-of-the-week&quot;&gt;Thought of the Week:&lt;/h2&gt;
&lt;p&gt;The United Nations General Assembly is meeting for its 74th session in New York City this month. The main talks will take place between Sept. 24 and 30 in Manhattan. Leaders and dignitaries from 193 countries will be meeting for solving the world’s most pressing problems. If you are in New York and want to feel useful, here are a couple of events you should go to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;New York City Climate Strike with Greta Thunberg &amp;amp; Puerto Rico Day of Action [8]&lt;/li&gt;
  &lt;li&gt;One of the world’s largest democracies, India, is committing major human rights violations in the UN-disputed territory of Kashmir, using state-funded propaganda to demonize and dehumanize a whole population of Kashmiri Muslims in the eyes of the common masses of India and the international community is simply looking the other way due to India’s economic and diplomatic presence. Find out more about this &lt;a href=&quot;https://www.facebook.com/StandWithKashmir/videos/2502005720029594/?__xts__[0]=68.ARDNgWb3kDSSzyIPCZaRklZ2YXOuED0gPPKK0YQVAwwDv76s1a8YgW7Efqhx3KmKVG54Ql411rKc9tyN_wbAld4OgTuL22vkusgJLlSwKspZ4z51K8SYIrEiOKhnOX2x5dwHT6h4wMnE9J-zt26yruQts80mWMqCnepldQjMNgJEbS9J1bGqM05kZsweSe3U5TdLU1Gvr69MhhktIzNYhqVE1CSrlIlSdFdD39VvfXqcTVLGdZsDJTbF17epk5NOTwj-ER2C-M6Wraij0MgW2hkB4vMsEUjPYxuKxDFYLOE-2lXvlyz_wsF3Jf7vk5n_kSkSObtS5WhV4G8vTFIimyiNxkT-DA2aD2xpUA&amp;amp;__tn__=-R&quot;&gt; here.&lt;/a&gt; Join &lt;strong&gt;Stand With Kashmir&lt;/strong&gt; &lt;a href=&quot;https://www.facebook.com/events/891671154542666/&quot;&gt;&lt;/a&gt; on Sept 27, 2019 as they protest in front of the UN headquarters to register their dissent as India’s prime minister comes to attend the session. [9]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;http://openingpathways.org/people-are-not-problems&quot;&gt;People are not problems; yet often are essential for solutions&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&quot;https://www.nature.com/news/governance-learn-from-diy-biologists-1.19507&quot;&gt;Governance: Learn from DIY biologists, Nature&lt;/a&gt;&lt;br /&gt;
[3] &lt;a href=&quot;https://fellowsblog.ted.com/how-citizen-science-bridges-the-gap-between-science-and-society-d693af125ae4&quot;&gt;Science to the People, TED Fellows&lt;/a&gt;&lt;br /&gt;
[4] &lt;a href=&quot;http://www.nightscout.info&quot;&gt;Nightscout, CGM on the Cloud&lt;/a&gt;&lt;br /&gt;
[5] &lt;a href=&quot;https://www.genspace.org&quot;&gt;Genspace&lt;/a&gt;&lt;br /&gt;
[6] &lt;a href=&quot;https://www.webopedia.com/TERM/A/architecture_of_participation.html&quot;&gt;Architecture of Participation, WeboPedia&lt;/a&gt;&lt;br /&gt;
[7] &lt;a href=&quot;https://opensource.com/business/12/6/architecture-participation&quot;&gt;An architecture of participation, opensource&lt;/a&gt;&lt;br /&gt;
[8] &lt;a href=&quot;https://actionnetwork.org/events/new-york-city-climate-strike-with-greta-thunberg&quot;&gt;Climate change protest&lt;/a&gt;&lt;br /&gt;
[9] &lt;a href=&quot;https://www.facebook.com/events/891671154542666/&quot;&gt;Kashmir Protest&lt;/a&gt;&lt;/p&gt;

</content>
		</entry>
	
		<entry>
			<title>Episode 2: seq2seq Models</title>
			<link href="http://hadiahameed.github.io/blog/2019/09/12/episode2"/>
			<updated>2019-09-12T00:00:00-04:00</updated>
			<id>http://hadiahameed.github.io/blog/2019/09/12/episode2</id>
			<content type="html">&lt;p&gt;This week I listened to a podcast on &lt;em&gt;“seq2seq models”&lt;/em&gt; on &lt;a href=&quot;https://player.fm/series/data-skeptic/seq2seq&quot;&gt;Data Skeptic [1]&lt;/a&gt;, and this is everything I learned from it:&lt;/p&gt;

&lt;p&gt;[Background: Data Skeptic is a show that is hosted by Kyle Polich who is a data scientist. In his podcasts, Kyle explains high level machine learning and data science concepts to his wife, Linh Da Tran, who does not have a computer science background. Sometimes their pet parrot Yoshi also gets dragged into the discussions and gets slain in the name of science!]&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary:&lt;/h2&gt;
&lt;p&gt;seq2seq models were first introduced by Google for Machine Translation [7]. It provides an end-to-end solution in automated machine translation (e.g. English to Russian translation). Instead of converting each individual word to its target word separately, it uses grammer and context to translate a sentence in one language to another. It has an encoder-decoder architecture with Recurrent Neural Networks (RNN). A variable sized input sequence is encoded into a fixed sized “thought vector” and passed onto the decoder which decodes it to output the translated sentence. Other applications of seq2seq models include, Text Summarization, Conversational Modeling, and Image Captioning [3].&lt;/p&gt;

&lt;h2 id=&quot;concepts&quot;&gt;Concepts:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What advantage do decision trees have over logistic regression?&lt;/span&gt;&lt;br /&gt;
Logistic regression does not have the ability to capture non-linear features in the data. Tree-based models divide the space into smaller and smaller regions and can better capture the division between classes, provided they are well-separated. More on this here [2].&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What is a common problem that many techniques in Machine Learning suffer from?&lt;/span&gt;&lt;br /&gt;
Most machine learning techniques rely on a fixed input and fixed output size. e.g. if you want to predict if a car would need maintenance on a certain part, you would need a set of features like mileage, output of a recent diagnostic test etc. The model is then trained on these features. We can only predict the output to the degree of the information content we have.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;A common joke in Natural Language Processing (NLP):&lt;/span&gt;&lt;br /&gt;
One time a group of data scientists attempted to use Mahcine Learning to translate the English sentence “The spirit is willing but the flesh is weak” to Russian. It came back as “The vodka is strong but the meat is rotten.” This shows how one-to-one translation is not a good solution in NLP. Context is important!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What is a thought vector in seq2seq models?&lt;/span&gt;&lt;br /&gt;
In Machine translation we will have an input (e.g. English) and an output (e.g. Russian) but the model also has an internal numerical representation that the model sets according to the input. It only exists in the &lt;strong&gt;tensor space&lt;/strong&gt;. It has no real meaning to anyone but it is specific to the algorithm/model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:blue&quot;&gt;Features of seq2seq models&lt;/span&gt;
    &lt;ol&gt;
      &lt;li&gt;Adaptive.&lt;/li&gt;
      &lt;li&gt;Variable sized input/output.&lt;/li&gt;
      &lt;li&gt;Slides along the input sequence (just like a DNA clamp sliding along a sequence during DNA replication.)&lt;/li&gt;
      &lt;li&gt;Maintains an internal state representation of the input sequence (an encoding) that it is constantly learning.&lt;/li&gt;
      &lt;li&gt;Has two pieces: An encoder and a decoder. Encoder encodes the input sentence into a thought vector. The decoder decodes the thought vector into the output language.&lt;/li&gt;
      &lt;li&gt;Structure: Input Layer -&amp;gt; Embedding Layer -&amp;gt; LSTM -&amp;gt; Dense Layer -&amp;gt; Output Layer&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:blue&quot;&gt;What are some applications of seq2seq models?&lt;/span&gt;   &lt;br /&gt;
Translation, Image captioning, Text summarization etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Seq2Seq Model animation [3]
&lt;img src=&quot;https://3.bp.blogspot.com/-3Pbj_dvt0Vo/V-qe-Nl6P5I/AAAAAAAABQc/z0_6WtVWtvARtMk0i9_AtLeyyGyV6AI4wCLcB/s1600/nmt-model-fast.gif&quot; alt=&quot;&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Seq2Seq Model [4]
&lt;img src=&quot;https://miro.medium.com/max/2658/1*Ismhi-muID5ooWf3ZIQFFg.png&quot; alt=&quot;&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;code-of-the-week&quot;&gt;Code of the Week:&lt;/h2&gt;
&lt;p&gt;This week I played with polynomial regression using different degrees to estimate C02 emissions from engine size. I calculated R2 score to compare the models. The source of the data is Government of Canada [5] and can be downloaded from [6]. Check out the coding exercise on my &lt;a href=&quot;https://github.com/hadiahameed/Data-science-blog/tree/master/Episode2-Regression&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt; profile.&lt;/p&gt;

&lt;h2 id=&quot;thought-of-the-week&quot;&gt;Thought of the Week:&lt;/h2&gt;
&lt;p&gt;I don’t know why I have so willfully ignored and underestimated the power of maintaining an active github account for the past 6 years of my engineering life. I remember being introduced to it during a summer internship when I was a sophomore at college, and also my careless use of git push and pull commands, my pulpy sophomore brain not quite wrapping around the version control theory as much as my mentor wanted it to. Keeping an active Github, I have come to realize, is nothing short of maintaining a coding journal (and I, who pride myself on owning more writing journals than I can ever manage, should have appreciated that sooner.)&lt;/p&gt;

&lt;p&gt;So now I have decided to do small, self-assigned projects and post them on my github account occasionally, adding my own two cents to this deep, rich well of knowledge that exists freely on the web (totally ignoring the cool, seasoned computer scientists who are dismissively snorting at me right now for getting all giddy with excitement over discovering github so late in my life!)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://player.fm/series/data-skeptic/seq2seq&quot; target=&quot;_blank&quot;&gt;se2seq, Data Skeptic&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&quot;https://blog.bigml.com/2016/09/28/logistic-regression-versus-decision-trees/&quot; target=&quot;_blank&quot;&gt;Logistic Regression versus Decision Trees, BigML&lt;/a&gt; &lt;br /&gt;
[3] &lt;a href=&quot;https://google.github.io/seq2seq/&quot; target=&quot;_blank&quot;&gt;seq2seq models, Google Github&lt;/a&gt; &lt;br /&gt;
[4] &lt;a href=&quot;https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d&quot; target=&quot;_blank&quot;&gt;Sequence to sequence model: Introduction and concepts&lt;/a&gt;  &lt;br /&gt;
[5] &lt;a href=&quot;https://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64&quot; target=&quot;_blank&quot;&gt;Government of Canada&lt;/a&gt; &lt;br /&gt;
[6] &lt;a href=&quot;https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv&quot; target=&quot;_blank&quot;&gt;Dataset&lt;/a&gt; &lt;br /&gt;
[7] &lt;a href=&quot;https://arxiv.org/abs/1609.08144&quot; target=&quot;_blank&quot;&gt;Google Neural Machine Translation&lt;/a&gt;&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>Episode 1: Multi-output forecasting</title>
			<link href="http://hadiahameed.github.io/blog/2019/09/05/episode1"/>
			<updated>2019-09-05T00:00:00-04:00</updated>
			<id>http://hadiahameed.github.io/blog/2019/09/05/episode1</id>
			<content type="html">&lt;p&gt;This week I read the paper &lt;em&gt;“Deep Multi-Output Forecasting - Learning to Accurately Predict Blood Glucose Trajectories”&lt;/em&gt;  [2], and this is everything I learned from it:&lt;/p&gt;

&lt;p&gt;[Disclaimer: Although the paper is a bit disappointing for an applications paper on diabetes, since it lacks some crucial details about study design, computational time, demographics, offline/online training/testing speed, however, I was still able to salvage certain important things from the wreckage.]&lt;/p&gt;

&lt;h2 id=&quot;vocabulary&quot;&gt;Vocabulary:&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Signal-step forecasting problem&lt;/strong&gt;: Estimate a future value of the signal for a single time instant using past values. e.g. blood glucose measurement.&lt;br /&gt;
&lt;strong&gt;Multi-step forecasting problem&lt;/strong&gt;: Estimating multiple values within a future time horizon, recursively.&lt;br /&gt;
&lt;strong&gt;Multi-output forecasting&lt;/strong&gt;: Estimating multiple values within a future time horizon, all at the same time instead of   recursively.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;This paper proposes a multi-ouput forecasting technique to predict blood glucose levels, using Recurrent Neural Networks (RNN).&lt;/li&gt;
  &lt;li&gt;The authors use real-time data of 40 patients with 550k blood glucose measurements recorded via continuous glucose monitor (CGM) over the period of three years.&lt;/li&gt;
  &lt;li&gt;Apart from using a many-to-many RNN, they have also used polynomial estimation in order to predict the underlying generative function of the signal. Overall, they propose four novel techniques which they compare with five baseline approaches including linear extrapolation, random forest and recursive RNNs.&lt;/li&gt;
  &lt;li&gt;While choosing the loss function, they claim that previous works related to speech generation show that it is beneficial to treat multi-output forecasting problems as multi-class classification, and therefore, instead of predicting actual future values, the model predicts the probability mass function (PMF) over discretized values of the signal. Consequently, they use cross-entropy loss as the performance metric. The class showing the highest probability is considered as the predicted class.&lt;/li&gt;
  &lt;li&gt;For train/test split, instead of a random split, they do a temporal split by adding the entirety of the final recording session to the test set and the second last session to the validation set, while rest of the data remains in the training set.&lt;/li&gt;
  &lt;li&gt;The hyperparameter search space included model depth, recurrent layer size, initial learning rate and input normalization. + In the Results section, they make comparisons between deep vs shallow approaches as well as multi-output versus recursive models, concluding that deep multi-output models outperform other baseline models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;concepts&quot;&gt;Concepts:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;Why does multi-step forecasting (MSF) often have a poor long-term performance?&lt;/span&gt;&lt;br /&gt;
In MSF, we use the current prediction to make the next prediction recursively and any error introduced at one stage can enter a positive feedback loop (accumulate over time).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What is the problem with multi-output forecasting?&lt;/span&gt;&lt;br /&gt;
It may not adequately capture dependencies among output predictions because it estimates all the future output values at the same time, instead of recursively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What is the difference between standard neural networks and RNNs?&lt;/span&gt;&lt;br /&gt;
Standarad Neural networks require fixed sized input but RNNs allow for variable sized inputs (becasue they encode the input signal into a fixed-sized, intermediate representation, sometimes referred to as the “thought vector”)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What is the difference between autoregressive (ARN) and RNN?&lt;/span&gt;&lt;br /&gt;
RNNs use &lt;em&gt;all&lt;/em&gt; the previous outputs for predicting the future value through hidden states.&lt;br /&gt;
ARN use a &lt;em&gt;limited&lt;/em&gt; number of past values given directly as input to the next cell. It is like any typical feed-forward network. There is an interesting set of blogposts explaining this concept [3],[4].&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;RNN architecture [7]&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ARN architecture [6]&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://miro.medium.com/max/1722/1*SAuwuiKBhzR4tBhr54mYkA.png&quot; alt=&quot;&quot; align=&quot;center&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif&quot; alt=&quot;&quot; align=&quot;center&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What is ARIMA and how is it used for time-series forecasting?&lt;/span&gt;   &lt;br /&gt;
ARIMA stands for &lt;em&gt;Autoregressive integrated moving average&lt;/em&gt; [5] and it uses the following techniques:  &lt;br /&gt;
&lt;strong&gt;AR - Auto-regressive&lt;/strong&gt;: captures dependencies between present and past signal values.&lt;br /&gt;
&lt;strong&gt;I - Integrated&lt;/strong&gt;: Makes the time-series stationary by differentiating the observations. &lt;br /&gt;
&lt;strong&gt;MA - Moving average&lt;/strong&gt;: Takes a moving average of the observations and uses residual error ( = expected_value –    predicted_value)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;color:blue&quot;&gt;What is early stopping in machine learning?&lt;/span&gt;&lt;br /&gt;
It is a form of &lt;em&gt;implicit regularization&lt;/em&gt; that is done to achieve a balance between underfitting and overfitting in order to improve generalization. Approaches:&lt;br /&gt;
(1) Change the number of epochs. (Drawback: One has to train and discard several models for each value of epoch in order to find the best epoch.)&lt;br /&gt;
(2) If the performance on validation set start degrading after a particular epoch, stop the training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In polynomial estimation, higher degree polynomials allow for better approximations of prediction windows but they also produce high variations in the output, causing the errors to accumulate over time and degrade the overall performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the paper, Poly multi-output (PolyMO) outperforms DeepMO, which shows that rephrasing &lt;em&gt;value&lt;/em&gt; forecasting problem as 
&lt;em&gt;function&lt;/em&gt; forecasting problem improves the performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;code-of-the-week&quot;&gt;Code of the Week:&lt;/h2&gt;
&lt;p&gt;This week I used an LSTM model in keras to predict the minimum daily temperature in Melbourne, Austrailia. I observed the effect of different activation functions on the model’s performance. Feel free to play with other parameters such as the optimizer, epochs, batch_size etc. The source of the data is Australian Bureau of Meteorology and can be downloaded from [8]. Check out the coding exercise on my &lt;a href=&quot;https://github.com/hadiahameed/Data-science-blog/tree/master/Episode1-Time-series-forecasting-LSTM&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt; profile.&lt;/p&gt;

&lt;h2 id=&quot;thought-of-the-week&quot;&gt;Thought of the Week:&lt;/h2&gt;
&lt;p&gt;The two major steps in learning the basics of any new algorithm:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Know the vocabulary: After you have jumped from super easy videos like RNN for dummies to super complex ones with a lot of math, there is only so much that you can retain in the long run. Therefore, in order to truly internalize whatever you have learned, simply make a concise list of all the terms and concepts related to that algorithm and imagine yourself explaining them during a hypothetical interview or a class you could be teaching in the future.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Code it and tweak it extensively in a programming language such as Python. Play with every parameter of that function and see how it blows up.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I think Jason@Machine Learning Mastery [1] is an excellent resource and follows the exact same approach of teaching a concise theory and a quick coding exercise in designing its ML tutorials. I love reading it!&lt;/p&gt;

&lt;p&gt;See you next week!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://machinelearningmastery.com&quot; target=&quot;_blank&quot;&gt;Machine Learning Mastery&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&quot;https://arxiv.org/abs/1806.05357&quot; target=&quot;_blank&quot;&gt;Deep Multi-output Forecasting&lt;/a&gt; &lt;br /&gt;
[3] &lt;a href=&quot;https://bair.berkeley.edu/blog/2018/08/06/recurrent/&quot; target=&quot;_blank&quot;&gt;Recurrent Neural Networks&lt;/a&gt; &lt;br /&gt;
[4] &lt;a href=&quot;https://eigenfoo.xyz/deep-autoregressive-models/&quot; target=&quot;_blank&quot;&gt;Autoregressive Models&lt;/a&gt;  &lt;br /&gt;
[5] &lt;a href=&quot;https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/&quot; target=&quot;_blank&quot;&gt;ARIMA&lt;/a&gt;   &lt;br /&gt;
[6] &lt;a href=&quot;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&quot; target=&quot;_blank&quot;&gt;Deepmind&lt;/a&gt;  &lt;br /&gt;
[7] &lt;a href=&quot;https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359&quot; target=&quot;_blank&quot;&gt;LSTM&lt;/a&gt;&lt;br /&gt;
[8] &lt;a href=&quot;https://machinelearningmastery.com/time-series-datasets-for-machine-learning/&quot; target=&quot;_blank&quot;&gt;7 Time Series Datasets for Machine Learning&lt;/a&gt;&lt;/p&gt;
</content>
		</entry>
	

</feed>
